{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wynnliam/texture_generator/blob/honeycomb/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgV-GSiYs3B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Liam Wynn, 2/11/2019, Variational Autoendcoder Texture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcdF-giXs_B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx-BPwD5tTdr",
        "colab_type": "code",
        "outputId": "65b8cb45-227c-4049-9ea2-73562b8471b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# This is the standard way to load in data. If for some reason\n",
        "# drive.mount doesn't work, use the other method.\n",
        "texture_type = 'honeycombed'\n",
        "model_path = '/content/drive/My Drive/VAE_Textures/' + texture_type + '/'\n",
        "data_path = model_path + '/data/'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def normalize_datum(datum_as_np_array):\n",
        "  return datum_as_np_array / 255.0\n",
        "\n",
        "def load_example(image_path):\n",
        "  raw_datum = PIL.Image.open(image_path)\n",
        "  datum_as_np_array = np.array(raw_datum, dtype = np.float32)\n",
        "  return normalize_datum(datum_as_np_array)\n",
        "  \n",
        "# TODO: Add test/validation data\n",
        "def load_dataset():\n",
        "  training_data_path = data_path + '/training_data/'\n",
        "  image_names = os.listdir(training_data_path)\n",
        "  result = []\n",
        "  \n",
        "  for image_name in image_names:\n",
        "    result.append(load_example(training_data_path + image_name))\n",
        "    \n",
        "  return np.array(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLON6m-xt_xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Add method to load data from file here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af9Bj3kIzPEG",
        "colab_type": "code",
        "outputId": "c44d277a-e298-447c-8de3-1e4766156eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "dataset = load_dataset()\n",
        "\n",
        "for image in dataset:\n",
        "  print(np.shape(image))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(305, 379, 3)\n",
            "(640, 640, 3)\n",
            "(120, 160, 3)\n",
            "(427, 640, 3)\n",
            "(322, 215, 3)\n",
            "(283, 314, 3)\n",
            "(160, 221, 3)\n",
            "(318, 373, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5TOskKzV9Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_sample_from_example(example):\n",
        "  example_shape = np.shape(example)\n",
        "  \n",
        "  num_rows = example_shape[0] - EXAMPLE_SIZE\n",
        "  num_cols = example_shape[1] - EXAMPLE_SIZE\n",
        "  \n",
        "  start_row = np.random.randint(0, num_rows)\n",
        "  start_col = np.random.randint(0, num_cols)\n",
        "  \n",
        "  result = example[np.ix_(range(start_row, start_row + EXAMPLE_SIZE),\n",
        "                          range(start_col, start_col + EXAMPLE_SIZE),\n",
        "                          [0])\n",
        "                  ]\n",
        "  \n",
        "  return result\n",
        "\n",
        "def load_batch_from_dataset(dataset, batch_size=64):\n",
        "  data_shape = np.shape(dataset)\n",
        "  num_examples = data_shape[0]\n",
        "  result = []\n",
        "  \n",
        "  for i in range(batch_size):\n",
        "    example_index = np.random.randint(0, num_examples)\n",
        "    result.append(get_training_sample_from_example(dataset[example_index]))\n",
        "  \n",
        "  return np.array(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjC-_Umh7kR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "# In pixels\n",
        "EXAMPLE_SIZE = 64\n",
        "\n",
        "NUM_LATENT_UNITS = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPGWYKAJ8rk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(X_in, keep_prob):\n",
        "  with tf.variable_scope(\"encoder\", reuse=None):\n",
        "    l = tf.layers.conv2d(X_in, filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.conv2d(l, filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.conv2d(l, filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.flatten(l)\n",
        "    \n",
        "    # Not entirely sure how this computes mean/sd. The mean part I kind of get,\n",
        "    # but the sd seems to be half of whatever the mn is?\n",
        "    mn = tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    # I think this has to do with how z is computer. Typically,\n",
        "    # z = mn + sd^(1/2) * epsilon\n",
        "    #sd = 0.5 * tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    sd = tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    # epsilon is randomly sampled from a normal distribution. By doing this,\n",
        "    # we can perform back-prop on mn and sd, while still randomly sampling.\n",
        "    epsilon = tf.random_normal(tf.stack([tf.shape(l)[0], NUM_LATENT_UNITS]))\n",
        "    # Reparameterized: z = mean + epsilon * e^sd.\n",
        "    z  = mn + tf.multiply(epsilon, tf.exp(sd))\n",
        "        \n",
        "    return z, mn, sd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c8DzYrj8xYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(sampled_z, keep_prob):\n",
        "  with tf.variable_scope(\"decoder\", reuse=None):\n",
        "    l = tf.layers.dense(sampled_z, units=4 * 4 * 1024, activation=tf.nn.leaky_relu)\n",
        "    l = tf.reshape(l, [-1, 4, 4, 1024])\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=512, kernel_size=5, strides=2, padding='same', activation=tf.nn.relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=256, kernel_size=5, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=128, kernel_size=5, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=1, kernel_size=5, strides=2, padding='same', activation=tf.nn.sigmoid)\n",
        "    \n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzeNm_pl8zgS",
        "colab_type": "code",
        "outputId": "37879506-0d51-4ad3-9f0a-48feb4194415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "X_in = tf.placeholder(dtype=tf.float32, shape=[None, EXAMPLE_SIZE, EXAMPLE_SIZE, 1], name='X')\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=[None, EXAMPLE_SIZE, EXAMPLE_SIZE, 1], name='Y')\n",
        "Y_flat = tf.reshape(Y, shape=[-1, EXAMPLE_SIZE * EXAMPLE_SIZE])\n",
        "keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')\n",
        "\n",
        "sampled, mn, sd = encoder(X_in, keep_prob)\n",
        "dec  = decoder(sampled, keep_prob)\n",
        "\n",
        "print(sampled)\n",
        "print(dec)\n",
        "\n",
        "unreshaped = tf.reshape(dec, [-1, EXAMPLE_SIZE * EXAMPLE_SIZE])\n",
        "# Reconstruction loss. Y_flat is a flattened version of our Y (which is the\n",
        "# same as X_in) that we use to compare against a flattened version of the\n",
        "# decoder's output. We can use squared difference because P(X_in | dec)\n",
        "# is an isotropic gaussian (no idea what that means, but this is a property of\n",
        "# isotropic gaussians apparently).\n",
        "img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, Y_flat), 1)\n",
        "# I think this is like a closed-form version of KL divergence\n",
        "# between two multivariate gaussian distributions. Basically we want to\n",
        "# encourage our latent distribution to be as close to a normal distribution\n",
        "# as possible.\n",
        "latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * sd - tf.square(mn) - tf.exp(2.0 * sd), 1)\n",
        "# And, of course, the overall loss is the sum of the image loss and latent loss.\n",
        "loss = tf.reduce_mean(img_loss + latent_loss)\n",
        "optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"encoder/add:0\", shape=(?, 8), dtype=float32)\n",
            "Tensor(\"decoder/conv2d_transpose_3/Sigmoid:0\", shape=(?, 64, 64, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWu6MF4zde69",
        "colab_type": "code",
        "outputId": "cbd74ac6-2f60-4d01-aa26-b6916996ff41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Set up GPU running capabilities\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2RRovPdjtJ",
        "colab_type": "code",
        "outputId": "4a521a4b-bd2f-415d-da7e-2fe4c1c8a6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.Session(config=config)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(3001):\n",
        "  with tf.device('/gpu:0'): \n",
        "    batch = load_batch_from_dataset(dataset)\n",
        "    sess.run(optimizer, feed_dict = {X_in : batch, Y: batch, keep_prob: 0.8})\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print('epoch: ', i)\n",
        "    ls, d, i_ls, d_ls, mu, sigm = sess.run([loss, dec, img_loss, latent_loss, mn, sd], feed_dict={X_in: batch, Y: batch, keep_prob: 1.0})\n",
        "    print('loss: ', ls)\n",
        "    print('')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "loss:  284.83752\n",
            "\n",
            "epoch:  10\n",
            "loss:  281.35553\n",
            "\n",
            "epoch:  20\n",
            "loss:  264.523\n",
            "\n",
            "epoch:  30\n",
            "loss:  269.99805\n",
            "\n",
            "epoch:  40\n",
            "loss:  234.58981\n",
            "\n",
            "epoch:  50\n",
            "loss:  206.36395\n",
            "\n",
            "epoch:  60\n",
            "loss:  153.51457\n",
            "\n",
            "epoch:  70\n",
            "loss:  150.36302\n",
            "\n",
            "epoch:  80\n",
            "loss:  172.37888\n",
            "\n",
            "epoch:  90\n",
            "loss:  144.57495\n",
            "\n",
            "epoch:  100\n",
            "loss:  158.396\n",
            "\n",
            "epoch:  110\n",
            "loss:  131.33516\n",
            "\n",
            "epoch:  120\n",
            "loss:  135.1007\n",
            "\n",
            "epoch:  130\n",
            "loss:  141.48257\n",
            "\n",
            "epoch:  140\n",
            "loss:  119.19731\n",
            "\n",
            "epoch:  150\n",
            "loss:  158.30426\n",
            "\n",
            "epoch:  160\n",
            "loss:  144.48721\n",
            "\n",
            "epoch:  170\n",
            "loss:  177.26175\n",
            "\n",
            "epoch:  180\n",
            "loss:  137.02763\n",
            "\n",
            "epoch:  190\n",
            "loss:  136.978\n",
            "\n",
            "epoch:  200\n",
            "loss:  144.00241\n",
            "\n",
            "epoch:  210\n",
            "loss:  117.22205\n",
            "\n",
            "epoch:  220\n",
            "loss:  173.21045\n",
            "\n",
            "epoch:  230\n",
            "loss:  124.19166\n",
            "\n",
            "epoch:  240\n",
            "loss:  139.14236\n",
            "\n",
            "epoch:  250\n",
            "loss:  116.81865\n",
            "\n",
            "epoch:  260\n",
            "loss:  153.12805\n",
            "\n",
            "epoch:  270\n",
            "loss:  118.401146\n",
            "\n",
            "epoch:  280\n",
            "loss:  128.6437\n",
            "\n",
            "epoch:  290\n",
            "loss:  144.58607\n",
            "\n",
            "epoch:  300\n",
            "loss:  178.53299\n",
            "\n",
            "epoch:  310\n",
            "loss:  131.54938\n",
            "\n",
            "epoch:  320\n",
            "loss:  140.82745\n",
            "\n",
            "epoch:  330\n",
            "loss:  119.24196\n",
            "\n",
            "epoch:  340\n",
            "loss:  149.07224\n",
            "\n",
            "epoch:  350\n",
            "loss:  162.30417\n",
            "\n",
            "epoch:  360\n",
            "loss:  144.3288\n",
            "\n",
            "epoch:  370\n",
            "loss:  142.55939\n",
            "\n",
            "epoch:  380\n",
            "loss:  131.46893\n",
            "\n",
            "epoch:  390\n",
            "loss:  153.70786\n",
            "\n",
            "epoch:  400\n",
            "loss:  148.40259\n",
            "\n",
            "epoch:  410\n",
            "loss:  116.67515\n",
            "\n",
            "epoch:  420\n",
            "loss:  149.12802\n",
            "\n",
            "epoch:  430\n",
            "loss:  136.21507\n",
            "\n",
            "epoch:  440\n",
            "loss:  121.300125\n",
            "\n",
            "epoch:  450\n",
            "loss:  141.13072\n",
            "\n",
            "epoch:  460\n",
            "loss:  147.94638\n",
            "\n",
            "epoch:  470\n",
            "loss:  135.36444\n",
            "\n",
            "epoch:  480\n",
            "loss:  136.69911\n",
            "\n",
            "epoch:  490\n",
            "loss:  114.89374\n",
            "\n",
            "epoch:  500\n",
            "loss:  137.88698\n",
            "\n",
            "epoch:  510\n",
            "loss:  125.73015\n",
            "\n",
            "epoch:  520\n",
            "loss:  147.06775\n",
            "\n",
            "epoch:  530\n",
            "loss:  131.07866\n",
            "\n",
            "epoch:  540\n",
            "loss:  153.52823\n",
            "\n",
            "epoch:  550\n",
            "loss:  160.61578\n",
            "\n",
            "epoch:  560\n",
            "loss:  119.15827\n",
            "\n",
            "epoch:  570\n",
            "loss:  152.04858\n",
            "\n",
            "epoch:  580\n",
            "loss:  138.77187\n",
            "\n",
            "epoch:  590\n",
            "loss:  110.74929\n",
            "\n",
            "epoch:  600\n",
            "loss:  134.8913\n",
            "\n",
            "epoch:  610\n",
            "loss:  140.43188\n",
            "\n",
            "epoch:  620\n",
            "loss:  136.97829\n",
            "\n",
            "epoch:  630\n",
            "loss:  136.84802\n",
            "\n",
            "epoch:  640\n",
            "loss:  117.17361\n",
            "\n",
            "epoch:  650\n",
            "loss:  134.59717\n",
            "\n",
            "epoch:  660\n",
            "loss:  129.76611\n",
            "\n",
            "epoch:  670\n",
            "loss:  122.625244\n",
            "\n",
            "epoch:  680\n",
            "loss:  117.24362\n",
            "\n",
            "epoch:  690\n",
            "loss:  125.17571\n",
            "\n",
            "epoch:  700\n",
            "loss:  129.8017\n",
            "\n",
            "epoch:  710\n",
            "loss:  111.80245\n",
            "\n",
            "epoch:  720\n",
            "loss:  138.41696\n",
            "\n",
            "epoch:  730\n",
            "loss:  122.37081\n",
            "\n",
            "epoch:  740\n",
            "loss:  117.63098\n",
            "\n",
            "epoch:  750\n",
            "loss:  162.31058\n",
            "\n",
            "epoch:  760\n",
            "loss:  137.67548\n",
            "\n",
            "epoch:  770\n",
            "loss:  121.08656\n",
            "\n",
            "epoch:  780\n",
            "loss:  109.7827\n",
            "\n",
            "epoch:  790\n",
            "loss:  135.73521\n",
            "\n",
            "epoch:  800\n",
            "loss:  134.76619\n",
            "\n",
            "epoch:  810\n",
            "loss:  139.12796\n",
            "\n",
            "epoch:  820\n",
            "loss:  129.33759\n",
            "\n",
            "epoch:  830\n",
            "loss:  125.00139\n",
            "\n",
            "epoch:  840\n",
            "loss:  143.36974\n",
            "\n",
            "epoch:  850\n",
            "loss:  121.00627\n",
            "\n",
            "epoch:  860\n",
            "loss:  153.52719\n",
            "\n",
            "epoch:  870\n",
            "loss:  111.54014\n",
            "\n",
            "epoch:  880\n",
            "loss:  144.53369\n",
            "\n",
            "epoch:  890\n",
            "loss:  104.53578\n",
            "\n",
            "epoch:  900\n",
            "loss:  124.428566\n",
            "\n",
            "epoch:  910\n",
            "loss:  115.48117\n",
            "\n",
            "epoch:  920\n",
            "loss:  135.53198\n",
            "\n",
            "epoch:  930\n",
            "loss:  112.75627\n",
            "\n",
            "epoch:  940\n",
            "loss:  118.36032\n",
            "\n",
            "epoch:  950\n",
            "loss:  111.42275\n",
            "\n",
            "epoch:  960\n",
            "loss:  114.33877\n",
            "\n",
            "epoch:  970\n",
            "loss:  129.70113\n",
            "\n",
            "epoch:  980\n",
            "loss:  121.609344\n",
            "\n",
            "epoch:  990\n",
            "loss:  132.50752\n",
            "\n",
            "epoch:  1000\n",
            "loss:  100.39937\n",
            "\n",
            "epoch:  1010\n",
            "loss:  120.70735\n",
            "\n",
            "epoch:  1020\n",
            "loss:  101.289246\n",
            "\n",
            "epoch:  1030\n",
            "loss:  127.98034\n",
            "\n",
            "epoch:  1040\n",
            "loss:  125.24822\n",
            "\n",
            "epoch:  1050\n",
            "loss:  118.99005\n",
            "\n",
            "epoch:  1060\n",
            "loss:  90.754684\n",
            "\n",
            "epoch:  1070\n",
            "loss:  109.206696\n",
            "\n",
            "epoch:  1080\n",
            "loss:  112.637596\n",
            "\n",
            "epoch:  1090\n",
            "loss:  121.67723\n",
            "\n",
            "epoch:  1100\n",
            "loss:  122.2553\n",
            "\n",
            "epoch:  1110\n",
            "loss:  107.09839\n",
            "\n",
            "epoch:  1120\n",
            "loss:  111.61195\n",
            "\n",
            "epoch:  1130\n",
            "loss:  111.82989\n",
            "\n",
            "epoch:  1140\n",
            "loss:  114.68599\n",
            "\n",
            "epoch:  1150\n",
            "loss:  108.89515\n",
            "\n",
            "epoch:  1160\n",
            "loss:  110.300156\n",
            "\n",
            "epoch:  1170\n",
            "loss:  117.23046\n",
            "\n",
            "epoch:  1180\n",
            "loss:  95.80502\n",
            "\n",
            "epoch:  1190\n",
            "loss:  104.78994\n",
            "\n",
            "epoch:  1200\n",
            "loss:  98.08348\n",
            "\n",
            "epoch:  1210\n",
            "loss:  113.93977\n",
            "\n",
            "epoch:  1220\n",
            "loss:  92.02551\n",
            "\n",
            "epoch:  1230\n",
            "loss:  104.33282\n",
            "\n",
            "epoch:  1240\n",
            "loss:  93.64309\n",
            "\n",
            "epoch:  1250\n",
            "loss:  109.39536\n",
            "\n",
            "epoch:  1260\n",
            "loss:  88.88963\n",
            "\n",
            "epoch:  1270\n",
            "loss:  109.43466\n",
            "\n",
            "epoch:  1280\n",
            "loss:  98.182045\n",
            "\n",
            "epoch:  1290\n",
            "loss:  104.62071\n",
            "\n",
            "epoch:  1300\n",
            "loss:  97.92182\n",
            "\n",
            "epoch:  1310\n",
            "loss:  109.55165\n",
            "\n",
            "epoch:  1320\n",
            "loss:  105.58894\n",
            "\n",
            "epoch:  1330\n",
            "loss:  87.28775\n",
            "\n",
            "epoch:  1340\n",
            "loss:  98.536316\n",
            "\n",
            "epoch:  1350\n",
            "loss:  113.33888\n",
            "\n",
            "epoch:  1360\n",
            "loss:  72.54859\n",
            "\n",
            "epoch:  1370\n",
            "loss:  76.97894\n",
            "\n",
            "epoch:  1380\n",
            "loss:  88.70232\n",
            "\n",
            "epoch:  1390\n",
            "loss:  94.839584\n",
            "\n",
            "epoch:  1400\n",
            "loss:  90.372604\n",
            "\n",
            "epoch:  1410\n",
            "loss:  94.89336\n",
            "\n",
            "epoch:  1420\n",
            "loss:  96.533\n",
            "\n",
            "epoch:  1430\n",
            "loss:  76.22203\n",
            "\n",
            "epoch:  1440\n",
            "loss:  82.890526\n",
            "\n",
            "epoch:  1450\n",
            "loss:  84.08157\n",
            "\n",
            "epoch:  1460\n",
            "loss:  82.82333\n",
            "\n",
            "epoch:  1470\n",
            "loss:  81.13518\n",
            "\n",
            "epoch:  1480\n",
            "loss:  80.898445\n",
            "\n",
            "epoch:  1490\n",
            "loss:  101.42964\n",
            "\n",
            "epoch:  1500\n",
            "loss:  86.2956\n",
            "\n",
            "epoch:  1510\n",
            "loss:  95.509796\n",
            "\n",
            "epoch:  1520\n",
            "loss:  100.2254\n",
            "\n",
            "epoch:  1530\n",
            "loss:  91.16477\n",
            "\n",
            "epoch:  1540\n",
            "loss:  96.337524\n",
            "\n",
            "epoch:  1550\n",
            "loss:  78.88231\n",
            "\n",
            "epoch:  1560\n",
            "loss:  92.91269\n",
            "\n",
            "epoch:  1570\n",
            "loss:  93.085526\n",
            "\n",
            "epoch:  1580\n",
            "loss:  84.46777\n",
            "\n",
            "epoch:  1590\n",
            "loss:  95.822205\n",
            "\n",
            "epoch:  1600\n",
            "loss:  91.8073\n",
            "\n",
            "epoch:  1610\n",
            "loss:  90.50724\n",
            "\n",
            "epoch:  1620\n",
            "loss:  82.06482\n",
            "\n",
            "epoch:  1630\n",
            "loss:  79.014435\n",
            "\n",
            "epoch:  1640\n",
            "loss:  76.32091\n",
            "\n",
            "epoch:  1650\n",
            "loss:  70.75295\n",
            "\n",
            "epoch:  1660\n",
            "loss:  73.57959\n",
            "\n",
            "epoch:  1670\n",
            "loss:  79.972206\n",
            "\n",
            "epoch:  1680\n",
            "loss:  82.830574\n",
            "\n",
            "epoch:  1690\n",
            "loss:  82.115685\n",
            "\n",
            "epoch:  1700\n",
            "loss:  74.65213\n",
            "\n",
            "epoch:  1710\n",
            "loss:  107.01262\n",
            "\n",
            "epoch:  1720\n",
            "loss:  82.895004\n",
            "\n",
            "epoch:  1730\n",
            "loss:  86.879715\n",
            "\n",
            "epoch:  1740\n",
            "loss:  83.58262\n",
            "\n",
            "epoch:  1750\n",
            "loss:  79.99465\n",
            "\n",
            "epoch:  1760\n",
            "loss:  88.03168\n",
            "\n",
            "epoch:  1770\n",
            "loss:  81.82565\n",
            "\n",
            "epoch:  1780\n",
            "loss:  82.93701\n",
            "\n",
            "epoch:  1790\n",
            "loss:  92.67717\n",
            "\n",
            "epoch:  1800\n",
            "loss:  73.33842\n",
            "\n",
            "epoch:  1810\n",
            "loss:  84.09052\n",
            "\n",
            "epoch:  1820\n",
            "loss:  75.41475\n",
            "\n",
            "epoch:  1830\n",
            "loss:  83.11079\n",
            "\n",
            "epoch:  1840\n",
            "loss:  79.17612\n",
            "\n",
            "epoch:  1850\n",
            "loss:  74.88435\n",
            "\n",
            "epoch:  1860\n",
            "loss:  99.62413\n",
            "\n",
            "epoch:  1870\n",
            "loss:  82.308586\n",
            "\n",
            "epoch:  1880\n",
            "loss:  77.18924\n",
            "\n",
            "epoch:  1890\n",
            "loss:  76.59684\n",
            "\n",
            "epoch:  1900\n",
            "loss:  90.763794\n",
            "\n",
            "epoch:  1910\n",
            "loss:  85.20107\n",
            "\n",
            "epoch:  1920\n",
            "loss:  82.428696\n",
            "\n",
            "epoch:  1930\n",
            "loss:  73.750824\n",
            "\n",
            "epoch:  1940\n",
            "loss:  84.40509\n",
            "\n",
            "epoch:  1950\n",
            "loss:  82.17845\n",
            "\n",
            "epoch:  1960\n",
            "loss:  84.94707\n",
            "\n",
            "epoch:  1970\n",
            "loss:  71.44084\n",
            "\n",
            "epoch:  1980\n",
            "loss:  69.35251\n",
            "\n",
            "epoch:  1990\n",
            "loss:  84.226006\n",
            "\n",
            "epoch:  2000\n",
            "loss:  75.2107\n",
            "\n",
            "epoch:  2010\n",
            "loss:  85.54172\n",
            "\n",
            "epoch:  2020\n",
            "loss:  93.95058\n",
            "\n",
            "epoch:  2030\n",
            "loss:  65.59186\n",
            "\n",
            "epoch:  2040\n",
            "loss:  80.41596\n",
            "\n",
            "epoch:  2050\n",
            "loss:  79.06131\n",
            "\n",
            "epoch:  2060\n",
            "loss:  93.67963\n",
            "\n",
            "epoch:  2070\n",
            "loss:  92.69064\n",
            "\n",
            "epoch:  2080\n",
            "loss:  87.94233\n",
            "\n",
            "epoch:  2090\n",
            "loss:  86.22762\n",
            "\n",
            "epoch:  2100\n",
            "loss:  77.54619\n",
            "\n",
            "epoch:  2110\n",
            "loss:  79.701164\n",
            "\n",
            "epoch:  2120\n",
            "loss:  84.5199\n",
            "\n",
            "epoch:  2130\n",
            "loss:  70.551544\n",
            "\n",
            "epoch:  2140\n",
            "loss:  73.913826\n",
            "\n",
            "epoch:  2150\n",
            "loss:  82.44861\n",
            "\n",
            "epoch:  2160\n",
            "loss:  99.54855\n",
            "\n",
            "epoch:  2170\n",
            "loss:  73.58746\n",
            "\n",
            "epoch:  2180\n",
            "loss:  81.12733\n",
            "\n",
            "epoch:  2190\n",
            "loss:  79.91379\n",
            "\n",
            "epoch:  2200\n",
            "loss:  69.55657\n",
            "\n",
            "epoch:  2210\n",
            "loss:  70.9438\n",
            "\n",
            "epoch:  2220\n",
            "loss:  70.88063\n",
            "\n",
            "epoch:  2230\n",
            "loss:  93.56064\n",
            "\n",
            "epoch:  2240\n",
            "loss:  77.56586\n",
            "\n",
            "epoch:  2250\n",
            "loss:  73.09764\n",
            "\n",
            "epoch:  2260\n",
            "loss:  85.25977\n",
            "\n",
            "epoch:  2270\n",
            "loss:  79.543304\n",
            "\n",
            "epoch:  2280\n",
            "loss:  70.20654\n",
            "\n",
            "epoch:  2290\n",
            "loss:  75.065414\n",
            "\n",
            "epoch:  2300\n",
            "loss:  70.921555\n",
            "\n",
            "epoch:  2310\n",
            "loss:  71.96812\n",
            "\n",
            "epoch:  2320\n",
            "loss:  79.07007\n",
            "\n",
            "epoch:  2330\n",
            "loss:  75.793045\n",
            "\n",
            "epoch:  2340\n",
            "loss:  76.982025\n",
            "\n",
            "epoch:  2350\n",
            "loss:  71.76148\n",
            "\n",
            "epoch:  2360\n",
            "loss:  72.56562\n",
            "\n",
            "epoch:  2370\n",
            "loss:  75.30647\n",
            "\n",
            "epoch:  2380\n",
            "loss:  71.043884\n",
            "\n",
            "epoch:  2390\n",
            "loss:  78.98976\n",
            "\n",
            "epoch:  2400\n",
            "loss:  77.9707\n",
            "\n",
            "epoch:  2410\n",
            "loss:  92.886185\n",
            "\n",
            "epoch:  2420\n",
            "loss:  72.78166\n",
            "\n",
            "epoch:  2430\n",
            "loss:  78.9485\n",
            "\n",
            "epoch:  2440\n",
            "loss:  83.65454\n",
            "\n",
            "epoch:  2450\n",
            "loss:  94.5477\n",
            "\n",
            "epoch:  2460\n",
            "loss:  75.41005\n",
            "\n",
            "epoch:  2470\n",
            "loss:  82.53811\n",
            "\n",
            "epoch:  2480\n",
            "loss:  84.868744\n",
            "\n",
            "epoch:  2490\n",
            "loss:  60.6229\n",
            "\n",
            "epoch:  2500\n",
            "loss:  74.63021\n",
            "\n",
            "epoch:  2510\n",
            "loss:  76.257095\n",
            "\n",
            "epoch:  2520\n",
            "loss:  86.10742\n",
            "\n",
            "epoch:  2530\n",
            "loss:  75.08088\n",
            "\n",
            "epoch:  2540\n",
            "loss:  81.61978\n",
            "\n",
            "epoch:  2550\n",
            "loss:  78.52398\n",
            "\n",
            "epoch:  2560\n",
            "loss:  66.92813\n",
            "\n",
            "epoch:  2570\n",
            "loss:  82.91737\n",
            "\n",
            "epoch:  2580\n",
            "loss:  74.07202\n",
            "\n",
            "epoch:  2590\n",
            "loss:  81.24362\n",
            "\n",
            "epoch:  2600\n",
            "loss:  75.85875\n",
            "\n",
            "epoch:  2610\n",
            "loss:  83.32207\n",
            "\n",
            "epoch:  2620\n",
            "loss:  85.25821\n",
            "\n",
            "epoch:  2630\n",
            "loss:  74.00791\n",
            "\n",
            "epoch:  2640\n",
            "loss:  84.83172\n",
            "\n",
            "epoch:  2650\n",
            "loss:  82.70711\n",
            "\n",
            "epoch:  2660\n",
            "loss:  77.485916\n",
            "\n",
            "epoch:  2670\n",
            "loss:  76.39195\n",
            "\n",
            "epoch:  2680\n",
            "loss:  81.90634\n",
            "\n",
            "epoch:  2690\n",
            "loss:  76.599266\n",
            "\n",
            "epoch:  2700\n",
            "loss:  68.8459\n",
            "\n",
            "epoch:  2710\n",
            "loss:  76.34347\n",
            "\n",
            "epoch:  2720\n",
            "loss:  71.594894\n",
            "\n",
            "epoch:  2730\n",
            "loss:  79.06328\n",
            "\n",
            "epoch:  2740\n",
            "loss:  75.21414\n",
            "\n",
            "epoch:  2750\n",
            "loss:  82.692\n",
            "\n",
            "epoch:  2760\n",
            "loss:  63.081932\n",
            "\n",
            "epoch:  2770\n",
            "loss:  68.445595\n",
            "\n",
            "epoch:  2780\n",
            "loss:  68.13447\n",
            "\n",
            "epoch:  2790\n",
            "loss:  84.31842\n",
            "\n",
            "epoch:  2800\n",
            "loss:  83.955536\n",
            "\n",
            "epoch:  2810\n",
            "loss:  76.95896\n",
            "\n",
            "epoch:  2820\n",
            "loss:  71.22678\n",
            "\n",
            "epoch:  2830\n",
            "loss:  77.486465\n",
            "\n",
            "epoch:  2840\n",
            "loss:  82.102165\n",
            "\n",
            "epoch:  2850\n",
            "loss:  78.58714\n",
            "\n",
            "epoch:  2860\n",
            "loss:  71.11391\n",
            "\n",
            "epoch:  2870\n",
            "loss:  76.436066\n",
            "\n",
            "epoch:  2880\n",
            "loss:  82.25424\n",
            "\n",
            "epoch:  2890\n",
            "loss:  80.4733\n",
            "\n",
            "epoch:  2900\n",
            "loss:  61.122765\n",
            "\n",
            "epoch:  2910\n",
            "loss:  80.5668\n",
            "\n",
            "epoch:  2920\n",
            "loss:  76.43894\n",
            "\n",
            "epoch:  2930\n",
            "loss:  79.115135\n",
            "\n",
            "epoch:  2940\n",
            "loss:  68.020485\n",
            "\n",
            "epoch:  2950\n",
            "loss:  75.11896\n",
            "\n",
            "epoch:  2960\n",
            "loss:  66.85766\n",
            "\n",
            "epoch:  2970\n",
            "loss:  76.745026\n",
            "\n",
            "epoch:  2980\n",
            "loss:  81.38829\n",
            "\n",
            "epoch:  2990\n",
            "loss:  68.4744\n",
            "\n",
            "epoch:  3000\n",
            "loss:  77.22221\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJs6jRGWd3Zp",
        "colab_type": "code",
        "outputId": "63feebf1-83dc-4751-c004-da7334ff776c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Generate and save some examples\n",
        "output_path = model_path + 'outputs/generated/'\n",
        "\n",
        "print(output_path)\n",
        "\n",
        "num_generated_images = 10\n",
        "randoms = []\n",
        "for i in range(num_generated_images):\n",
        "  randoms.append(np.random.normal(0, 1, NUM_LATENT_UNITS))\n",
        "  \n",
        "randoms = np.array(randoms)\n",
        "with tf.device('/gpu:0'): \n",
        "  images = sess.run(dec, feed_dict = {sampled: randoms, keep_prob: 1.0})\n",
        "\n",
        "for j in range(num_generated_images):\n",
        "  result = images[j]\n",
        "  generated_image = np.repeat(result, 3, axis=2)\n",
        "  generated_image = np.array(generated_image * 255.0, dtype=np.uint8)\n",
        "  \n",
        "  save_image = PIL.Image.fromarray(generated_image)\n",
        "  save_image.save(output_path + str(j) + '.bmp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/VAE_Textures/honeycombed/outputs/generated/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}