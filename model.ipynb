{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wynnliam/texture_generator/blob/honeycomb/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgV-GSiYs3B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Liam Wynn, 2/11/2019, Variational Autoendcoder Texture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcdF-giXs_B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx-BPwD5tTdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the standard way to load in data. If for some reason\n",
        "# drive.mount doesn't work, use the other method.\n",
        "texture_type = 'honeycombed'\n",
        "model_path = '/content/drive/My Drive/VAE_Textures/' + texture_type + '/'\n",
        "data_path = model_path + '/data/'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def normalize_datum(datum_as_np_array):\n",
        "  return datum_as_np_array / 255.0\n",
        "\n",
        "def load_example(image_path):\n",
        "  raw_datum = PIL.Image.open(image_path)\n",
        "  datum_as_np_array = np.array(raw_datum, dtype = np.float32)\n",
        "  return normalize_datum(datum_as_np_array)\n",
        "  \n",
        "# TODO: Add test/validation data\n",
        "def load_dataset():\n",
        "  training_data_path = data_path + '/training_data/'\n",
        "  image_names = os.listdir(training_data_path)\n",
        "  result = []\n",
        "  \n",
        "  for image_name in image_names:\n",
        "    result.append(load_example(training_data_path + image_name))\n",
        "    \n",
        "  return np.array(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLON6m-xt_xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Add method to load data from file here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af9Bj3kIzPEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = load_dataset()\n",
        "\n",
        "for image in dataset:\n",
        "  print(np.shape(image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5TOskKzV9Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_sample_from_example(example):\n",
        "  example_shape = np.shape(example)\n",
        "  \n",
        "  num_rows = example_shape[0] - EXAMPLE_SIZE\n",
        "  num_cols = example_shape[1] - EXAMPLE_SIZE\n",
        "  \n",
        "  start_row = np.random.randint(0, num_rows)\n",
        "  start_col = np.random.randint(0, num_cols)\n",
        "  \n",
        "  result = example[np.ix_(range(start_row, start_row + EXAMPLE_SIZE),\n",
        "                          range(start_col, start_col + EXAMPLE_SIZE),\n",
        "                          [0])\n",
        "                  ]\n",
        "  \n",
        "  return result\n",
        "\n",
        "def load_batch_from_dataset(dataset, batch_size=64):\n",
        "  data_shape = np.shape(dataset)\n",
        "  num_examples = data_shape[0]\n",
        "  result = []\n",
        "  \n",
        "  for i in range(batch_size):\n",
        "    example_index = np.random.randint(0, num_examples)\n",
        "    result.append(get_training_sample_from_example(dataset[example_index]))\n",
        "  \n",
        "  return np.array(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjC-_Umh7kR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "# In pixels\n",
        "EXAMPLE_SIZE = 64\n",
        "\n",
        "NUM_LATENT_UNITS = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPGWYKAJ8rk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(X_in, keep_prob):\n",
        "  with tf.variable_scope(\"encoder\", reuse=None):\n",
        "    l = tf.layers.conv2d(X_in, filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.conv2d(l, filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.conv2d(l, filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    l = tf.layers.flatten(l)\n",
        "    \n",
        "    # Not entirely sure how this computes mean/sd. The mean part I kind of get,\n",
        "    # but the sd seems to be half of whatever the mn is?\n",
        "    mn = tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    # I think this has to do with how z is computer. Typically,\n",
        "    # z = mn + sd^(1/2) * epsilon\n",
        "    #sd = 0.5 * tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    sd = tf.layers.dense(l, units=NUM_LATENT_UNITS)\n",
        "    # epsilon is randomly sampled from a normal distribution. By doing this,\n",
        "    # we can perform back-prop on mn and sd, while still randomly sampling.\n",
        "    epsilon = tf.random_normal(tf.stack([tf.shape(l)[0], NUM_LATENT_UNITS]))\n",
        "    # Reparameterized: z = mean + epsilon * e^sd.\n",
        "    z  = mn + tf.multiply(epsilon, tf.exp(sd))\n",
        "        \n",
        "    return z, mn, sd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c8DzYrj8xYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(sampled_z, keep_prob):\n",
        "  with tf.variable_scope(\"decoder\", reuse=None):\n",
        "    l = tf.layers.dense(sampled_z, units=4 * 4 * 1024, activation=tf.nn.leaky_relu)\n",
        "    l = tf.reshape(l, [-1, 4, 4, 1024])\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=512, kernel_size=5, strides=2, padding='same', activation=tf.nn.relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=256, kernel_size=5, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=128, kernel_size=5, strides=2, padding='same', activation=tf.nn.leaky_relu)\n",
        "    l = tf.nn.dropout(l, keep_prob)\n",
        "    \n",
        "    l = tf.layers.conv2d_transpose(l, filters=1, kernel_size=5, strides=2, padding='same', activation=tf.nn.sigmoid)\n",
        "    \n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzeNm_pl8zgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "X_in = tf.placeholder(dtype=tf.float32, shape=[None, EXAMPLE_SIZE, EXAMPLE_SIZE, 1], name='X')\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=[None, EXAMPLE_SIZE, EXAMPLE_SIZE, 1], name='Y')\n",
        "Y_flat = tf.reshape(Y, shape=[-1, EXAMPLE_SIZE * EXAMPLE_SIZE])\n",
        "keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')\n",
        "\n",
        "sampled, mn, sd = encoder(X_in, keep_prob)\n",
        "dec  = decoder(sampled, keep_prob)\n",
        "\n",
        "print(sampled)\n",
        "print(dec)\n",
        "\n",
        "unreshaped = tf.reshape(dec, [-1, EXAMPLE_SIZE * EXAMPLE_SIZE])\n",
        "# Reconstruction loss. Y_flat is a flattened version of our Y (which is the\n",
        "# same as X_in) that we use to compare against a flattened version of the\n",
        "# decoder's output. We can use squared difference because P(X_in | dec)\n",
        "# is an isotropic gaussian (no idea what that means, but this is a property of\n",
        "# isotropic gaussians apparently).\n",
        "img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, Y_flat), 1)\n",
        "# I think this is like a closed-form version of KL divergence\n",
        "# between two multivariate gaussian distributions. Basically we want to\n",
        "# encourage our latent distribution to be as close to a normal distribution\n",
        "# as possible.\n",
        "latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * sd - tf.square(mn) - tf.exp(2.0 * sd), 1)\n",
        "# And, of course, the overall loss is the sum of the image loss and latent loss.\n",
        "loss = tf.reduce_mean(img_loss + latent_loss)\n",
        "optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWu6MF4zde69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up GPU running capabilities\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2RRovPdjtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(config=config)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(3001):\n",
        "  with tf.device('/gpu:0'): \n",
        "    batch = load_batch_from_dataset(dataset)\n",
        "    sess.run(optimizer, feed_dict = {X_in : batch, Y: batch, keep_prob: 0.8})\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print('epoch: ', i)\n",
        "    ls, d, i_ls, d_ls, mu, sigm = sess.run([loss, dec, img_loss, latent_loss, mn, sd], feed_dict={X_in: batch, Y: batch, keep_prob: 1.0})\n",
        "    print('loss: ', ls)\n",
        "    print('')\n",
        "  \n",
        "print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJs6jRGWd3Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate and save some examples\n",
        "output_path = model_path + 'outputs/generated/'\n",
        "\n",
        "print(output_path)\n",
        "\n",
        "num_generated_images = 10\n",
        "randoms = []\n",
        "for i in range(num_generated_images):\n",
        "  randoms.append(np.random.normal(0, 1, NUM_LATENT_UNITS))\n",
        "  \n",
        "randoms = np.array(randoms)\n",
        "with tf.device('/gpu:0'): \n",
        "  images = sess.run(dec, feed_dict = {sampled: randoms, keep_prob: 1.0})\n",
        "\n",
        "for j in range(num_generated_images):\n",
        "  result = images[j]\n",
        "  generated_image = np.repeat(result, 3, axis=2)\n",
        "  generated_image = np.array(generated_image * 255.0, dtype=np.uint8)\n",
        "  \n",
        "  save_image = PIL.Image.fromarray(generated_image)\n",
        "  save_image.save(output_path + str(j) + '.bmp')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}